{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4bd563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "''' Parameters (CHange Anything Here!) '''\n",
    "transform = transforms.ToTensor()\n",
    "batch_size = 3\n",
    "#lifetime Sparcity\n",
    "k_percent = 5\n",
    "\n",
    "\n",
    "''' Code Starts Here '''\n",
    "#Data MNIST\n",
    "mnist_data = datasets.MNIST(root='./data', train = True, download = True, transform = transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset= mnist_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "dataiter = iter(data_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "\n",
    "# testing model\n",
    "''' Conv 2d Layer \n",
    "#         Accessible Variables: .weights(Tensor), .bias(Tensor)\n",
    "#         parameters :\n",
    "#         torch.nn.Conv2d(in_channels, out_channels, \n",
    "#                         kernel_size, stride=1, padding=0, \n",
    "#                         dilation=1, groups=1, bias=True, \n",
    "#                         padding_mode='zeros')\n",
    "'''\n",
    "# CONV-WTA CRITERIA\n",
    "# - zero padded, so that each feature map has the same size as the input\n",
    "# - hidden representation is mapped linearly to the output using a deconvolution operation\n",
    "# - Parameters are optimized to reduce the mean squared error MSE\n",
    "# - Conv layer is 5 x5, DECONVOLUTION layer is using filters of 11x 11\n",
    "### In this implementation, I will not use deconvolution, but transpose convolution to ease process\n",
    "class Autoencoder_Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #Image size:N, 28, 28\n",
    "        self.conv1      = nn.Conv2d(1, 2, 5, stride=1, padding = 2) \n",
    "        self.transConv1 = nn.ConvTranspose2d(in_channels=2, out_channels=3, kernel_size=11, stride =1, padding = 5) # padding will decrease output size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.conv1(x) # encode, output: torch.Size([3, 2, 26, 26])\n",
    "        decoded = self.transConv1(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    # With ReLU\n",
    "#     def forward(self, x, relu = True):\n",
    "#         encoded = F.relu(self.conv1(x)) # encode, output: torch.Size([3, 2, 26, 26])\n",
    "#         x = encoded.view(-1, 2 * 26 * 26) # flattening it out\n",
    "#         decoded = self.decoder(x) \n",
    "#         decoded = decoded.view(3, 1, 28, 28) # converting it back to same format as input\n",
    "#         #encoded is the output of the layer\n",
    "#         return decoded\n",
    "    \n",
    "# class RMSELoss(nn.Module):\n",
    "#     def __init__(self, eps=1e-6):\n",
    "#         super().__init__()\n",
    "#         self.mse = nn.MSELoss()\n",
    "#         self.eps = eps\n",
    "\n",
    "#     def forward(self,yhat,y):\n",
    "#         loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "#         return loss\n",
    "    \n",
    "model = Autoencoder_Test()\n",
    "generator = model.parameters() #(returns a generator)\n",
    "# criterion = RMSELoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43931d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Testing block for transposed convolution maps\n",
    "img, labels = dataiter.next()\n",
    "encoded, decoded = model(img)\n",
    "print(encoded.size())\n",
    "print(decoded.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638ffc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Training loop\n",
    "# Hidden Units here is to be defined as feature maps\n",
    "# Spatial Sparsity: For every feature Filter, after batch prediction, pick the highest output activity winner and set the rest to 0\n",
    "# Lifetime Sparsity: For every feature Filter, after batch prediction, pick the hightst k% of all the winners picked in Spatial Sparsity\n",
    "from sortedcontainers import SortedList, SortedDict\n",
    "\n",
    "num_epochs = 1\n",
    "sorted_list = SortedList()\n",
    "winnersMap = {}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "#     for (img, labels) in data_loader:\n",
    "    img, labels = dataiter.next()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # First feedforward to get the k% of winners\n",
    "        feature_map = model(img) # returns the feature maps of all batch examples in order\n",
    "\n",
    "        # Summing up the activation maps to find the maximum activation hidden map from the the batch\n",
    "        summation = torch.sum(feature_map, (2, 3)) # reduce the 3rd and 4th dimension of the tensor. Summation is a 2-dim tensor\n",
    "        print(\"\\n\\n Sum of the feature maps(Should have batch_size values): \\n\", summation)\n",
    "\n",
    "        # batch_idx: torch tensor with the max batch index, size = num_features\n",
    "        # max_val:   torch tensor with the max_val for each batch, size = num_features\n",
    "        max_val, batch_idx = torch.max(summation, 0) # returns a tensor with the size of number of features\n",
    "        max_val = max_val.numpy()\n",
    "        batch_idx = batch_idx.numpy()\n",
    "        print(\"\\n\\nMaximum Values: \", max_val, \"\\nBatch Location Indexes: \", batch_idx)\n",
    "\n",
    "        # where feature_num starts from 0\n",
    "        for feature_num, max_values in enumerate(max_val):\n",
    "        # Have to store list of tuples in sorted dict where tuples = (feature no., index)\n",
    "        # if there is more than one value in this list, then backprop have to iterate through the list\n",
    "            if winnersMap.get(max_values) == None:\n",
    "                winnersMap[max_values] = [(feature_num, batch_idx[feature_num])]\n",
    "            else:\n",
    "                winnersMap[max_values] = winnersMap[max_values].append((feature_num, batch_idx[feature_num]))\n",
    "\n",
    "        sorted_dict = SortedDict(winnersMap) # store and the keys sort Automatically\n",
    "        print(sorted_dict)\n",
    "        \n",
    "        # Constructing the new Tensor with only the k% of the winners\n",
    "        # This tensor.... requires_grad = True?\n",
    "        winner batches = \n",
    "        \n",
    "    # 2nd feedforward bias with only the k% of winner batches with relu\n",
    "    k_forward = model(k_percent_winners, True)\n",
    "    loss.backward()\n",
    "    loss = criterion(decoded, img)+\n",
    "#     layers = model.children()\n",
    "#     hidden = next(layers)\n",
    "#     print(next(hidden.parameters()))\n",
    "# #     for params in hidden.parameters():\n",
    "# #         print(params.grad)\n",
    "        \n",
    "# #         for param in child.parameters():\n",
    "# #             if\n",
    "# #             param.grad = 0   \n",
    "\n",
    "    # Update weights\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "# Plan reduce 3 batches into 1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
