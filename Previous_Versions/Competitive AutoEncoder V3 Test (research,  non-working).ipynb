{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee25fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "''' Parameters (CHange Anything Here!) '''\n",
    "transform = transforms.ToTensor()\n",
    "batch_size = 3\n",
    "#lifetime Sparcity\n",
    "k_percent = 5\n",
    "\n",
    "\n",
    "''' Code Starts Here '''\n",
    "#Data MNIST\n",
    "mnist_data = datasets.MNIST(root='./data', train = True, download = True, transform = transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset= mnist_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "dataiter = iter(data_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "\n",
    "# testing model\n",
    "''' Conv 2d Layer \n",
    "#         Accessible Variables: .weights(Tensor), .bias(Tensor)\n",
    "#         parameters :\n",
    "#         torch.nn.Conv2d(in_channels, out_channels, \n",
    "#                         kernel_size, stride=1, padding=0, \n",
    "#                         dilation=1, groups=1, bias=True, \n",
    "#                         padding_mode='zeros')\n",
    "'''\n",
    "class Autoencoder_Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #Image size:N, 28, 28\n",
    "        self.conv1 = nn.Conv2d(1, 2, 3, stride=1) # stride 2 will reduce size by half (W - F + 2P)/\n",
    "        self.decoder = nn.Linear(2 * 26 * 26, 28*28) # input items, output items\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.conv1(x) # \n",
    "        print(\"\\nEncoder Output Size : \\n\", encoded.size())\n",
    "        x = encoded.view(-1, 2 * 26 * 26)\n",
    "        decoded = self.decoder(x)\n",
    "        decoded = decoded.view(3, 1, 28, 28)\n",
    "        #encoded is the output of the layer\n",
    "        return encoded, decoded\n",
    "    \n",
    "#     def decode (self, x):\n",
    "#         decoded = self.decoder(x)\n",
    "#         return decoded\n",
    "    \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss\n",
    "    \n",
    "model = Autoencoder_Test()\n",
    "generator = model.parameters() #(returns a generator)\n",
    "criterion = RMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dc1c9c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder Output Size : \n",
      " torch.Size([3, 2, 26, 26])\n",
      "torch.Size([3, 2, 26, 26])\n",
      "torch.Size([3, 28, 28])\n",
      "tensor([[[-0.0287,  0.0710,  0.2128,  ..., -0.1840,  0.1458,  0.1766],\n",
      "         [-0.1088,  0.1234,  0.2379,  ..., -0.2177,  0.0156,  0.0575],\n",
      "         [-0.0335, -0.1569,  0.4818,  ..., -0.0211,  0.2814,  0.4293],\n",
      "         ...,\n",
      "         [-0.0795, -0.0854,  0.1426,  ..., -0.0689,  0.0270, -0.1244],\n",
      "         [-0.0447,  0.0539,  0.0855,  ..., -0.2471, -0.0073, -0.0243],\n",
      "         [ 0.0219,  0.0731,  0.2498,  ...,  0.1524,  0.1936, -0.0947]],\n",
      "\n",
      "        [[ 0.0163, -0.0012,  0.1540,  ..., -0.1624,  0.0271,  0.1217],\n",
      "         [-0.0527,  0.1910,  0.1957,  ..., -0.2279, -0.0806,  0.2667],\n",
      "         [ 0.0214,  0.0258,  0.4219,  ...,  0.0884,  0.2397,  0.5031],\n",
      "         ...,\n",
      "         [-0.0099, -0.1998,  0.2910,  ..., -0.0292,  0.1540, -0.0778],\n",
      "         [-0.1572,  0.1589,  0.0561,  ..., -0.1509, -0.0122, -0.1615],\n",
      "         [ 0.0965,  0.0101,  0.3438,  ...,  0.1533,  0.1266, -0.1582]],\n",
      "\n",
      "        [[-0.0633,  0.0558,  0.2524,  ..., -0.3240,  0.2001,  0.3941],\n",
      "         [-0.1086,  0.0535,  0.2051,  ..., -0.1601,  0.0682,  0.0112],\n",
      "         [-0.0609, -0.2251,  0.4149,  ..., -0.1432,  0.2317,  0.5080],\n",
      "         ...,\n",
      "         [-0.0335, -0.0904,  0.1515,  ..., -0.0028,  0.1567, -0.0853],\n",
      "         [-0.2305,  0.0859,  0.1617,  ..., -0.2113, -0.0081,  0.0159],\n",
      "         [ 0.0804, -0.0258,  0.3074,  ...,  0.1457,  0.2001, -0.1131]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "torch.Size([3, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#Testing for model output size\n",
    "img, _ = dataiter.next()\n",
    "encoded, decoded = model(img)\n",
    "print(encoded.size())\n",
    "print(decoded.size())\n",
    "print(decoded)\n",
    "print(img.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77003126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Linear(in_features=1568, out_features=784, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Testing cell for freezing gradients\n",
    "layers = model.children() # returns a generator\n",
    "hidden = next(layers)\n",
    "fcl = next(layers)\n",
    "print(hidden) \n",
    "print(fcl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae46d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x00000245543A14A0>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(hidden.parameters())\n",
    "for params in hidden.parameters():\n",
    "    print(params.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "138587d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-6cf0277c17bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#     for (img, labels) in data_loader:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mfeature_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# returns the feature maps of all batch examples in order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m#     decoded = model.decode(feature_map)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-e10853286e7d>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m#encoded is the output of the layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m#         print(\"\\nEncoder Output Size : \\n\", encoded.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;31m#     def decode (self, x):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decoded' is not defined"
     ]
    }
   ],
   "source": [
    "# Batch Training loop\n",
    "# Hidden Units here is to be defined as feature maps\n",
    "# Spatial Sparsity: For every feature Filter, after batch prediction, pick the highest output activity winner and set the rest to 0\n",
    "# Lifetime Sparsity: For every feature Filter, after batch prediction, pick the hightst k% of all the winners picked in Spatial Sparsity\n",
    "from sortedcontainers import SortedList, SortedDict\n",
    "\n",
    "num_epochs = 1\n",
    "sorted_list = SortedList()\n",
    "winnersMap = {}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "#     for (img, labels) in data_loader:\n",
    "    img, labels = dataiter.next()\n",
    "    feature_map = model(img) # returns the feature maps of all batch examples in order\n",
    "#     decoded = model.decode(feature_map)\n",
    "    loss = criterion(feature_map, img)\n",
    "    loss.backward()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         # Summing up the activation maps to find the maximum activation hidden map from the the batch\n",
    "#         summation = torch.sum(feature_map, (2, 3)) # reduce the 3rd and 4th dimension of the tensor. Summation is a 2-dim tensor\n",
    "#         print(\"\\n\\n Sum of the feature maps(Should have batch_size values): \\n\", summation)\n",
    "\n",
    "#         # batch_idx: torch tensor with the max batch index, size = num_features\n",
    "#         # max_val:   torch tensor with the max_val for each batch, size = num_features\n",
    "#         max_val, batch_idx = torch.max(summation, 0) # returns a tensor with the size of number of features\n",
    "#         max_val = max_val.numpy()\n",
    "#         batch_idx = batch_idx.numpy()\n",
    "#         print(\"\\n\\nMaximum Values: \", max_val, \"\\nBatch Location Indexes: \", batch_idx)\n",
    "\n",
    "#         # where feature_num starts from 0\n",
    "#         for feature_num, max_values in enumerate(max_val):\n",
    "#         # Have to store list of tuples in sorted dict where tuples = (feature no., index)\n",
    "#         # if there is more than one value in this list, then backprop have to iterate through the list\n",
    "#             if winnersMap.get(max_values) == None:\n",
    "#                 winnersMap[max_values] = [(batch_idx[feature_num], feature_num)]\n",
    "#             else:\n",
    "#                 winnersMap[max_values] = winnersMap[max_values].append((batch_idx[feature_num], feature_num))\n",
    "\n",
    "#         sorted_dict = SortedDict(winnersMap) # store and the keys sort Automatically\n",
    "#         print(sorted_dict)\n",
    "        \n",
    "#     Set the .grad attribute of the hidden units who are not winners to 0\n",
    "#     for i in range( ((k_percent/100)*num_features).floor() ):\n",
    "    \n",
    "    # calculating the gradient freezing the gradient of the hidden layer\n",
    "    \n",
    "    layers = model.children()\n",
    "    hidden = next(layers)\n",
    "    for params in hidden.parameters():\n",
    "        print(params.grad)\n",
    "        \n",
    "#         for param in child.parameters():\n",
    "#             if\n",
    "#             param.grad = 0   \n",
    "\n",
    "    # Update weights\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
