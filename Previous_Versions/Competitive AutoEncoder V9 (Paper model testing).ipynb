{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4e049611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "# from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "# import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "''' Device config'''\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "''' Parameters (CHange Anything Here!) '''\n",
    "torch.manual_seed(10)\n",
    "transform = transforms.ToTensor()\n",
    "batch_size = 100\n",
    "# lifetime Sparcity\n",
    "k_rate = 0.5\n",
    "\n",
    "''' Code Starts Here '''\n",
    "# Data MNIST\n",
    "mnist_data = datasets.MNIST(root='./data', train = True, download = True, transform = transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset= mnist_data, batch_size = batch_size, shuffle = True)\n",
    "dataiter = iter(data_loader)\n",
    "\n",
    "\n",
    "''' Conv 2d Layer \n",
    "#         Accessible Variables: .weights(Tensor), .bias(Tensor)\n",
    "#         parameters :\n",
    "#         torch.nn.Conv2d(in_channels, out_channels, \n",
    "#                         kernel_size, stride=1, padding=0, \n",
    "#                         dilation=1, groups=1, bias=True, \n",
    "#                         padding_mode='zeros')\n",
    "'''\n",
    "# CONV-WTA CRITERIA\n",
    "# - zero padded, so that each feature map has the same size as the input\n",
    "# - hidden representation is mapped linearly to the output using a deconvolution operation\n",
    "# - Parameters are optimized to reduce the mean squared error MSE\n",
    "# - Conv layer is 5 x5, DECONVOLUTION layer is using filters of 11x 11\n",
    "### In this implementation, I will not use deconvolution, but transpose convolution to ease process\n",
    "class Competitive_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #Image size:N, 28, 28\n",
    "        self.conv1      = nn.Conv2d(1, 128, 5, stride=1, padding = 2)  # size:N, 28, 28\n",
    "        self.conv2      = nn.Conv2d(128, 128, 5, stride=1, padding = 2)  # size:N, 28, 28\n",
    "        self.transConv1 = nn.ConvTranspose2d(in_channels=128, out_channels=1, kernel_size=11, stride =1, padding = 5) # padding will decrease output size # size:N, 28, 28\n",
    "#         self.conv1.apply(self.init_weights)\n",
    "#         self.conv2.apply(self.init_weights)\n",
    "#         self.transConv1.apply(self.init_weights)\n",
    "        \n",
    "#     def init_weights(m):\n",
    "#         torch.nn.init.xavier_uniform(m.weight)\n",
    "#         m.bias.data.fill_(0.01)    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.conv1(x) # encode, output: torch.Size([num_batches, num_channels, 28, 28])\n",
    "        hidden = self.conv2(encoded)\n",
    "        b4hidden = hidden.clone()\n",
    "        winner = self.spatial_sparsity_(hidden)\n",
    "#         print(hidden.shape)\n",
    "#         hidden = self.lifetime_sparsity_(hidden, winner, k_rate)\n",
    "        decoded = self.transConv1(hidden)\n",
    "        return encoded, b4hidden, hidden, decoded\n",
    "\n",
    "#     def reconstruct(self, x):\n",
    "#         encoded = nn.ReLU(self.conv1(x)) # encode, output: torch.Size([3, 2, 28, 28])\n",
    "#         decoded = torch.sigmoid(self.transConv1(hidden))\n",
    "#         return decoded\n",
    "    \n",
    "    # Spatial Sparsity reconstructs the activation map, remain only one winner neuron of each feature map and rest to 0\n",
    "    # with torch.no_grad() temporarily sets all of the requires_grad flags to false\n",
    "    def spatial_sparsity_(self, hiddenMaps):\n",
    "        with torch.no_grad():\n",
    "            shape = hiddenMaps.shape  #torch.Size([batch_size, feature_num, 26, 26])\n",
    "            n_batches = shape[0]\n",
    "            n_features = shape[1]\n",
    "            size = shape[2]\n",
    "            \n",
    "            # Step 1: flatten it out, find max_vals\n",
    "            flatten = hiddenMaps.view(n_batches, n_features, -1)\n",
    "            maxval, _ = torch.max(flatten, 2) # max_val return size[n_batches, n_features]\n",
    "            \n",
    "            # Step 2: creating \"drop\" Array to be multiplied into featureMaps, dropping loser values\n",
    "            maxval_p = torch.reshape(maxval, (n_batches, n_features, 1, 1))\n",
    "            drop = torch.where(hiddenMaps < maxval_p, \n",
    "                               torch.zeros((n_batches, n_features, size, size)).to(device), \n",
    "                               torch.ones((n_batches,n_features, size, size)).to(device))\n",
    "        \n",
    "        # To retain the graph, use .data to only modify the data of the tensor\n",
    "        hiddenMaps.data = hiddenMaps.data*drop.data\n",
    "        return maxval\n",
    "        \n",
    "    # Only retain the top-k percent of the winners for every feature. The rest will be zeroed out\n",
    "    def lifetime_sparsity_(self, hiddenMaps, maxval, k_percent):\n",
    "        with torch.no_grad():\n",
    "            shape = hiddenMaps.shape  #torch.Size([batch_size, feature_num, 26, 26])\n",
    "            n_batches = shape[0]\n",
    "            n_features = shape[1]\n",
    "            size = shape[2]\n",
    "            k = math.floor(n_batches * k_percent)\n",
    "            \n",
    "            top_k, _ = torch.topk(maxval, k, 0) \n",
    "\n",
    "            # Step 2: creating \"drop\" Array to be multiplied into featureMaps, dropping loser values\n",
    "            drop = torch.where(maxval < top_k[k-1:k, :],  \n",
    "                               torch.zeros((n_batches, n_features)).to(device), \n",
    "                               torch.ones((n_batches, n_features)).to(device))\n",
    "\n",
    "        # To retain the graph, use .data to only modify the data of the tensor\n",
    "        hiddenMaps.data = hiddenMaps.data * drop.reshape(n_batches, n_features, 1, 1).data\n",
    "        return hiddenMaps\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aa4501e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded:  <MkldnnConvolutionBackward object at 0x000001A4145B6100>\n",
      "Hidden:  <MkldnnConvolutionBackward object at 0x000001A411CCC5E0>\n",
      "Decoded:  <SlowConvTranspose2DBackward object at 0x000001A4145B6100>\n"
     ]
    }
   ],
   "source": [
    "model = Competitive_Autoencoder().to(device)\n",
    "img, _ = next(dataiter)\n",
    "\n",
    "encoded, b4hidden, hidden, decoded = model(img)\n",
    "# kernels = hidden[0:1:,0:1,:,:].detach().clone()\n",
    "# print(kernels.shape)\n",
    "# filter_img = torchvision.utils.make_grid(kernels, nrow = 12)\n",
    "# # # change ordering since matplotlib requires images to \n",
    "# # # be (H, W, C)\n",
    "# print(filter_img.shape)\n",
    "# plt.imshow(filter_img.permute(1, 2, 0))\n",
    "# # plt.subplot(2, 9, 9)\n",
    "# # plt.imshow(decoded[0][0].detach())\n",
    "print(\"Encoded: \", encoded.grad_fn)\n",
    "# print(\"Hiddenb4: \", b4hidden)\n",
    "print(\"Hidden: \", hidden.grad_fn)\n",
    "print(\"Decoded: \", decoded.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3804540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.26152986]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(b4hidden[0][0].detach())\n",
    "# print(b4hidden[0][0][0:20, 0:20])\n",
    "npu=b4hidden[0][0][5:10, 15:20].detach().numpy()\n",
    "print(np.where(npu>0.26, npu, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3eeb56ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a4146f6af0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(hidden[0][0][5:10, 15:20].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenMaps = encoded\n",
    "with torch.no_grad():\n",
    "    shape = hiddenMaps.shape  #torch.Size([batch_size, feature_num, 28, 28])\n",
    "    n_batches = shape[0]\n",
    "    n_features = shape[1]\n",
    "    size = shape[2]\n",
    "\n",
    "    # Step 1: flatten it out, find max_vals\n",
    "    flatten = hiddenMaps.view(n_batches, n_features, -1)\n",
    "    maxval, _ = torch.max(flatten, 2) # max_val return size[n_batches, n_features]\n",
    "\n",
    "    # Step 2: creating \"drop\" Array to be multiplied into featureMaps, dropping loser values\n",
    "    maxval_p = torch.reshape(maxval, (n_batches, n_features, 1, 1))\n",
    "    drop = torch.where(hiddenMaps < maxval_p, \n",
    "                       torch.zeros((n_batches, n_features, size, size)).to(device), \n",
    "                       torch.ones((n_batches,n_features, size, size)).to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
