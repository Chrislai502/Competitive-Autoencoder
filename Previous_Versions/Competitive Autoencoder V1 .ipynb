{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6114bbfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c77b2571d4b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m '''\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "By : chris-wei-xun.lai\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "import matplotllib.pyplot as plt\n",
    "\n",
    "### Parameters ### (Change anything here)\n",
    "\n",
    "sparsity_rate = 5 # in percentage\n",
    "batch_size = 5\n",
    "num_epochs = 5\n",
    "\n",
    "##########################################\n",
    "\n",
    "### DATASET ###\n",
    "\n",
    "# MNIST handwriting Digits\n",
    "mnist_data = datasets.MNIST(root = './data', train = True, download = True, transform = transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset = mnist_data, batch_size = 64, shuffle = True)\n",
    "\n",
    "# LED Dataset (Remember to normalize the input data to [0,1])\n",
    "\n",
    "\n",
    "dataiter = iter(data_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64378f66",
   "metadata": {},
   "source": [
    "## Initializing weights\n",
    "First is to have a function that initialize linear layer weights as we define each layer  \n",
    "Source: \n",
    "- https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "\n",
    "Code:\n",
    "```python\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
    "net.apply(init_weights)\n",
    "\n",
    "```\n",
    "<br>\n",
    "\n",
    "---  \n",
    "Second, to tweak the weights after we're done creating all the layers <br>\n",
    "Source: \n",
    "- https://www.youtube.com/watch?v=nA6oEAE9IVc&list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&index=88\n",
    "- https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L11/code/weight_normal.ipynb\n",
    "\n",
    "Code:\n",
    "```python\n",
    "class MultilayerPerceptron(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_classes, drop_proba, \n",
    "                 num_hidden_1, num_hidden_2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.my_network = torch.nn.Sequential(\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(num_features, num_hidden_1),\n",
    "            torch.nn.ReLU(),\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            torch.nn.ReLU(),\n",
    "            # output layer\n",
    "            torch.nn.Linear(num_hidden_2, num_classes)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                m.weight.detach().normal_(0, 0.001)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.detach().zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.my_network(x)\n",
    "        return logits\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeabf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL ###\n",
    "# Competitive autoencoder\n",
    "\n",
    "class CompAutoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        # Initialize super class\n",
    "        super(CompAutoencoder, self).__init__()\n",
    "        \n",
    "        # Method to initialize weights using Kaiming He method, where a = sqrt(6/input_size)\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                _a = sqrt(6/input_size)\n",
    "                torch.nn.init.kaiming_uniform_(m.weight, a = _a, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                # torch.nn.init.xavier_uniform(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "                                     nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "                                     nn.ReLU()\n",
    "                                     )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "                                     nn.Linear(2, 2)\n",
    "                                     nn.Sigmoid() #Because the input in 0 and 1, the output should also be between 0 and 1\n",
    "                                     )\n",
    "        \n",
    "        self.decoder.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "model = CompAutoencoder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e58b3",
   "metadata": {},
   "source": [
    "## Creating your own Loss Functions\n",
    "source: \n",
    "- https://discuss.pytorch.org/t/rmse-loss-function/16540/3\n",
    "        \n",
    "```python\n",
    "# You should be careful with NaN which will appear if the mse=0. Something like this would probably be better :\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "#loss = torch.sqrt(criterion(x, y))\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867aad38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
