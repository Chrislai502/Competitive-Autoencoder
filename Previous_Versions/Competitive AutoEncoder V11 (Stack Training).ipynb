{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab651a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "# from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "''' Device config'''\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "''' Parameters (CHange Anything Here!) '''\n",
    "transform = transforms.ToTensor()\n",
    "batch_size = 150\n",
    "# lifetime Sparcity\n",
    "k_rate = 0.05\n",
    "\n",
    "''' Conv 2d Layer \n",
    "#         Accessible Variables: .weights(Tensor), .bias(Tensor)\n",
    "#         parameters :\n",
    "#         torch.nn.Conv2d(in_channels, out_channels, \n",
    "#                         kernel_size, stride=1, padding=0, \n",
    "#                         dilation=1, groups=1, bias=True, \n",
    "#                         padding_mode='zeros')\n",
    "'''\n",
    "# CONV-WTA CRITERIA\n",
    "# - zero padded, so that each feature map has the same size as the input\n",
    "# - hidden representation is mapped linearly to the output using a deconvolution operation\n",
    "# - Parameters are optimized to reduce the mean squared error MSE\n",
    "# - Conv layer is 5 x5, DECONVOLUTION layer is using filters of 11x 11\n",
    "### In this implementation, I will not use deconvolution, but transpose convolution to ease process\n",
    "class Competitive_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #Image size:N, 28, 28\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 81, 5, stride=1, padding = 2),     \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(81),\n",
    "#             nn.Conv2d(81, 81, 5, stride=1, padding = 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm2d(81),\n",
    "            nn.Conv2d(81, 81, 5, stride=1, padding = 2),\n",
    "            nn.BatchNorm2d(81)\n",
    "        )\n",
    "        self.transConv1 = nn.ConvTranspose2d(in_channels=81, out_channels=1, kernel_size=11, stride =1, padding = 5) # padding will decrease output size # size:N, 28, 28\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        winner = self.spatial_sparsity_(encoded)\n",
    "        self.lifetime_sparsity_(encoded, winner, k_rate)\n",
    "        decoded = self.transConv1(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    # Spatial Sparsity reconstructs the activation map, remain only one winner neuron of each feature map and rest to 0\n",
    "    # with torch.no_grad() temporarily sets all of the requires_grad flags to false\n",
    "    def spatial_sparsity_(self, hiddenMaps):\n",
    "        with torch.no_grad():\n",
    "            shape = hiddenMaps.shape  #torch.Size([batch_size, feature_num, 26, 26])\n",
    "            n_batches = shape[0]\n",
    "            n_features = shape[1]\n",
    "            size = shape[2]\n",
    "            \n",
    "            # Step 1: flatten it out, find max_vals\n",
    "            flatten = hiddenMaps.view(n_batches, n_features, -1)\n",
    "            maxval, _ = torch.max(flatten, 2) # max_val return size[n_batches, n_features]\n",
    "            \n",
    "            # Step 2: creating \"drop\" Array to be multiplied into featureMaps, dropping loser values\n",
    "            maxval_p = torch.reshape(maxval, (n_batches, n_features, 1, 1))\n",
    "            drop = torch.where(hiddenMaps < maxval_p, \n",
    "                               torch.zeros((n_batches, n_features, size, size)).to(device), \n",
    "                               torch.ones((n_batches,n_features, size, size)).to(device))\n",
    "        \n",
    "        # To retain the graph, use .data to only modify the data of the tensor\n",
    "        hiddenMaps.data = hiddenMaps.data*drop.data\n",
    "        return maxval\n",
    "        \n",
    "    # Only retain the top-k percent of the winners for every feature. The rest will be zeroed out\n",
    "    def lifetime_sparsity_(self, hiddenMaps, maxval, k_percent):\n",
    "        with torch.no_grad():\n",
    "            shape = hiddenMaps.shape  #torch.Size([batch_size, feature_num, 26, 26])\n",
    "            n_batches = shape[0]\n",
    "            n_features = shape[1]\n",
    "            size = shape[2]\n",
    "            k = 10\n",
    "            \n",
    "            top_k, _ = torch.topk(maxval, k, 0) \n",
    "\n",
    "            # Step 2: creating \"drop\" Array to be multiplied into featureMaps, dropping loser values\n",
    "            drop = torch.where(maxval < top_k[k-1:k, :],  \n",
    "                               torch.zeros((n_batches, n_features)).to(device), \n",
    "                               torch.ones((n_batches, n_features)).to(device))\n",
    "\n",
    "        # To retain the graph, use .data to only modify the data of the tensor\n",
    "        hiddenMaps.data = hiddenMaps.data * drop.reshape(n_batches, n_features, 1, 1).data\n",
    "        \n",
    "# Saving the state dicts of the model\n",
    "def save_model_optimizer(model, optimizer = None, filename = \"CompAutoModel\", path = \"\"):\n",
    "    temp_device = torch.device('cpu')\n",
    "    if optimizer == None:\n",
    "        torch.save({\n",
    "            'model_state': model.state_dict()\n",
    "        }, path+filename+\".pth\")\n",
    "    else:\n",
    "        torch.save({\n",
    "            'model_state': model.state_dict(),\n",
    "            'optim_state': optimizer.state_dict()\n",
    "        }, path+filename+\".pth\")\n",
    "    \n",
    "def load_model(filename, path = \"\"):\n",
    "    checkpoint = torch.load(path+filename+\".pth\", map_location = 'cpu')\n",
    "    model_obj = Competitive_Autoencoder()\n",
    "    model_obj.load_state_dict(checkpoint['model_state'])\n",
    "    model_obj.eval()\n",
    "        \n",
    "    if len(checkpoint.keys()) == 1:\n",
    "        return model_obj\n",
    "    else:\n",
    "        optim_obj = torch.optim.Adam(model_obj.parameters(), lr = 0)\n",
    "        optim_obj.load_state_dict(checkpoint['optim_state'])\n",
    "        return model_obj, optim_obj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
