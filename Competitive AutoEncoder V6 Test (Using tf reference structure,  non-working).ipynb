{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac0b29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "''' Parameters (CHange Anything Here!) '''\n",
    "transform = transforms.ToTensor()\n",
    "batch_size = 3\n",
    "# lifetime Sparcity\n",
    "k_percent = 5\n",
    "\n",
    "\n",
    "''' Code Starts Here '''\n",
    "# Data MNIST\n",
    "mnist_data = datasets.MNIST(root='./data', train = True, download = True, transform = transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset= mnist_data, batch_size = batch_size, shuffle = True)\n",
    "dataiter = iter(data_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "\n",
    "# testing model\n",
    "''' Conv 2d Layer \n",
    "#         Accessible Variables: .weights(Tensor), .bias(Tensor)\n",
    "#         parameters :\n",
    "#         torch.nn.Conv2d(in_channels, out_channels, \n",
    "#                         kernel_size, stride=1, padding=0, \n",
    "#                         dilation=1, groups=1, bias=True, \n",
    "#                         padding_mode='zeros')\n",
    "'''\n",
    "# CONV-WTA CRITERIA\n",
    "# - zero padded, so that each feature map has the same size as the input\n",
    "# - hidden representation is mapped linearly to the output using a deconvolution operation\n",
    "# - Parameters are optimized to reduce the mean squared error MSE\n",
    "# - Conv layer is 5 x5, DECONVOLUTION layer is using filters of 11x 11\n",
    "### In this implementation, I will not use deconvolution, but transpose convolution to ease process\n",
    "class Autoencoder_Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #Image size:N, 28, 28\n",
    "        self.conv1      = nn.Conv2d(1, 2, 5, stride=1, padding = 2) \n",
    "        self.transConv1 = nn.ConvTranspose2d(in_channels=2, out_channels=3, kernel_size=11, stride =1, padding = 5) # padding will decrease output size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.conv1(x) # encode, output: torch.Size([3, 2, 26, 26])\n",
    "        hidden, winners = self.spatial_sparsity_(encoded)\n",
    "        hidden = self.lifetime_sparsity_(hidden, winner, k_percent = 0.1)\n",
    "        decoded = self.transConv1(hidden)\n",
    "        return decoded\n",
    "    \n",
    "    # Spatial Sparsity reconstructs the activation map, remain only one winner neuron of each feature map and rest to 0\n",
    "    # with torch.no_grad() temporarily sets all of the requires_grad flags to false\n",
    "    def spatial_sparsity_(self, hiddenMaps):\n",
    "        with torch.no_grad():\n",
    "            shape = hiddenMaps.shape  #torch.Size([batch_size, feature_num, 26, 26])\n",
    "            n_batches = shape[0]\n",
    "            n_features = shape[1]\n",
    "            size = shape[2]\n",
    "            \n",
    "            # Step 1: flatten it out, find max_vals\n",
    "            flatten = hiddenMaps.view(n_batches, n_features, -1)\n",
    "            max_val, batch_idx = torch.max(summation, 0) # max_val return size[n_batches, n_features]\n",
    "            \n",
    "            # Step 2: creating \"drop\" Array to be multiplied into featureMaps, dropping loser values\n",
    "            maxval, _ = torch.max(flatten, 2)\n",
    "            maxval_p = torch.reshape(maxval, (n_batches, n_features, 1, 1))\n",
    "            drop = torch.where(torch.reshape(b, (n_batches, n_features, size, size)) < maxval_p, \n",
    "                               torch.zeros((n_batches, n_features, size, size)), \n",
    "                               torch.ones((n_batches,n_features2, size, size)))\n",
    "            \n",
    "            return hiddenMaps*drop, maxval\n",
    "    # Only retain the top-k percent of the winners for every feature. The rest will be zeroed out\n",
    "    def lifetime_sparsity_(self, hiddenMaps, maxval, k_percent):\n",
    "        with torch.no_grad():\n",
    "            shape = hiddenMaps.shape  #torch.Size([batch_size, feature_num, 26, 26])\n",
    "            n_batches = shape[0]\n",
    "            n_features = shape[1]\n",
    "            size = shape[2]\n",
    "            k = floor(n_batches * k_percent)\n",
    "            \n",
    "            # Step 1: pick the max of dim-0, along the batch axis\n",
    "            # if input size is (n, c), returns (k, c) if operate over dim-0\n",
    "            top_k, _ = torch.topk(maxval, k, 0)\n",
    "            winner_t = torch.transpose(a, 0, 1)\n",
    "            \n",
    "            # Step 2: creating \"drop\" Array to be multiplied into featureMaps, dropping loser values\n",
    "            drop = torch.where(winner_t < top_k[:,k-1:k], \n",
    "                               torch.zeros((n_batches, n_features, size, size)), \n",
    "                               torch.ones((n_batches, n_features, size, size)))\n",
    "            \n",
    "            # dropping all them loser batches to zero\n",
    "            return hiddenMaps * drop.reshape(n_batches, n_features, 1, 1)\n",
    "    \n",
    "def _lifetime_sparsity(self, h, winner, rate):\n",
    "    shape = tf.shape(winner)\n",
    "    n = shape[0]\n",
    "    c = shape[1]\n",
    "    k = tf.cast(rate * tf.cast(n, tf.float32), tf.int32)\n",
    "\n",
    "    winner = tf.transpose(winner) # c, n\n",
    "    th_k, _ = tf.nn.top_k(winner, k) # c, k\n",
    "\n",
    "    shape_t = tf.stack([c, n])\n",
    "    drop = tf.where(winner < th_k[:,k-1:k], # c, n\n",
    "      tf.zeros(shape_t, tf.float32), tf.ones(shape_t, tf.float32))\n",
    "    drop = tf.transpose(drop) # n, c\n",
    "    return h * tf.reshape(drop, tf.stack([n, 1, 1, c]))\n",
    "    \n",
    "model = Autoencoder_Test()\n",
    "generator = model.parameters() #(returns a generator)\n",
    "# criterion = RMSELoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46994830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow of the example code in tf:\n",
    "def loss(self, x, lifetime_sparsity=0.20):\n",
    "    h = self.encoder(x)\n",
    "    h, winner = self._spatial_sparsity(h)\n",
    "    h = self._lifetime_sparsity(h, winner, lifetime_sparsity)\n",
    "    y = self._decoder(h)\n",
    "    \n",
    "# def _spatial_sparsity(self, h):\n",
    "#     shape = tf.shape(h)\n",
    "#     n = shape[0]\n",
    "#     c = shape[3]\n",
    "\n",
    "#     h_t = tf.transpose(h, [0, 3, 1, 2]) # n, c, h, w\n",
    "#     h_r = tf.reshape(h_t, tf.stack([n, c, -1])) # n, c, h*w\n",
    "\n",
    "#     th, _ = tf.nn.top_k(h_r, 1) # n, c, 1\n",
    "#     th_r = tf.reshape(th, tf.stack([n, 1, 1, c])) # n, 1, 1, c\n",
    "#     drop = tf.where(h < th_r, \n",
    "#       tf.zeros(shape, tf.float32), tf.ones(shape, tf.float32))\n",
    "\n",
    "#     # spatially dropped & winner\n",
    "#     return h*drop, tf.reshape(th, tf.stack([n, c])) # n, c\n",
    "\n",
    "def _lifetime_sparsity(self, h, winner, rate):\n",
    "    shape = tf.shape(winner)\n",
    "    n = shape[0]\n",
    "    c = shape[1]\n",
    "    k = tf.cast(rate * tf.cast(n, tf.float32), tf.int32)\n",
    "\n",
    "    winner = tf.transpose(winner) # c, n\n",
    "    th_k, _ = tf.nn.top_k(winner, k) # c, k\n",
    "\n",
    "    shape_t = tf.stack([c, n])\n",
    "    drop = tf.where(winner < th_k[:,k-1:k], # c, n\n",
    "      tf.zeros(shape_t, tf.float32), tf.ones(shape_t, tf.float32))\n",
    "    drop = tf.transpose(drop) # n, c\n",
    "    return h * tf.reshape(drop, tf.stack([n, 1, 1, c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7185b089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32) tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor([4 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1. 3. 5. 7.]\n",
      " [2. 4. 6. 8.]], shape=(2, 4), dtype=float32)\n",
      "topk:\n",
      " tf.Tensor(\n",
      "[[7. 5.]\n",
      " [8. 6.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[5.]\n",
      " [6.]], shape=(2, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[6],\n",
       "        [7],\n",
       "        [8]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fix the seed to have same outputs\n",
    "import tensorflow as tf\n",
    "import math as m\n",
    "torch.manual_seed(10)\n",
    "\n",
    "# torch.max experimentation\n",
    "# a = torch.rand(4,4)\n",
    "# print(a)\n",
    "# print(a.view(-1, 4*4)) #same\n",
    "# print(a.view(1, -1))   #same\n",
    "# flatten = a.view(1, -1)\n",
    "# maxval, idx = torch.max(flatten, 1)\n",
    "# print(maxval, idx)\n",
    "# print(\"y: \", m.floor(idx.item()/4)) # y axis, or dim - 0\n",
    "# print(\"x: \",idx.item() %4)             # x-axis, or dim - 1\n",
    "\n",
    "size = 4\n",
    "\n",
    "#create array\n",
    "b = torch.rand(3, 2, size, size)\n",
    "# print(b)\n",
    "b.view(3, 2, -1)\n",
    "flatten = b.view(3, 2, -1)\n",
    "# print(flatten)\n",
    "maxval, idx = torch.max(flatten, 2)\n",
    "# print(maxval.shape)\n",
    "\n",
    "maxval = torch.reshape(maxval, (3, 2, 1, 1))\n",
    "# print(maxval)\n",
    "\n",
    "test = torch.reshape(b, (3, 2, size, size)) < maxval\n",
    "# print(test)\n",
    "\n",
    "drop = torch.where(torch.reshape(b, (3, 2, size, size)) < maxval, \n",
    "                   torch.zeros((3, 2, size, size)), \n",
    "                   torch.ones((3, 2, size, size)))\n",
    "\n",
    "# print(drop*b)\n",
    "# print(drop)\n",
    "\n",
    "# # Setting all the existing non-zero loder values into 0\n",
    "# - flow\n",
    "# shape = tf.shape(h) initial shape  GUESS: (n, h, w, c)\n",
    "# n = number of batches?\n",
    "# c = number of features?\n",
    "# transpose(hidden, [0, 3, 1, 2]) # n, c, h, w (changing the order?)\n",
    "# reshape(transposed hidden, stack([n, c, -1])) # n, c, h*w\n",
    "# th, _ = get_top(reshaped hidden, one value) # n, 1, 1, c\n",
    "\n",
    "# Lifetime sparsity flow\n",
    "t = tf.constant([[1.0, 2.0], \n",
    "                 [3.0, 4.0], \n",
    "                 [5.0, 6.0], \n",
    "                 [7.0, 8.0]])\n",
    "rate = 0.5\n",
    "shape = tf.shape(t)\n",
    "n = shape[0]\n",
    "c = shape[1]\n",
    "print(shape)\n",
    "print(n, c)\n",
    "k = tf.cast(rate * tf.cast(n, tf.float32), tf.int32)\n",
    "print(k)\n",
    "print(tf.stack([n,c]))\n",
    "t = tf.transpose(t) # c, n\n",
    "print(t)\n",
    "th_k, _ = tf.nn.top_k(t, k) # c, k\n",
    "print(\"topk:\\n\", th_k)\n",
    "print(th_k[:,k-1:k])\n",
    "# shape_t = tf.stack([c, n])\n",
    "# drop = tf.where(winner < th_k[:,k-1:k], # c, n\n",
    "#   tf.zeros(shape_t, tf.float32), tf.ones(shape_t, tf.float32))\n",
    "# drop = tf.transpose(drop) # n, c\n",
    "# return h * tf.reshape(drop, tf.stack([n, 1, 1, c]))\n",
    "k = torch.tensor(2)\n",
    "a = torch.arange(4*3).view(4, 3) #(n, c)\n",
    "a = torch.transpose(a, 0, 1) #(c, n)\n",
    "# print(a)\n",
    "topk, _ = torch.topk(a, k, 1)\n",
    "# print(topk)\n",
    "\n",
    "topk[: , k-1:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fb07b472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11],\n",
      "          [12, 13, 14, 15]],\n",
      "\n",
      "         [[16, 17, 18, 19],\n",
      "          [20, 21, 22, 23],\n",
      "          [24, 25, 26, 27],\n",
      "          [28, 29, 30, 31]],\n",
      "\n",
      "         [[32, 33, 34, 35],\n",
      "          [36, 37, 38, 39],\n",
      "          [40, 41, 42, 43],\n",
      "          [44, 45, 46, 47]]],\n",
      "\n",
      "\n",
      "        [[[48, 49, 50, 51],\n",
      "          [52, 53, 54, 55],\n",
      "          [56, 57, 58, 59],\n",
      "          [60, 61, 62, 63]],\n",
      "\n",
      "         [[64, 65, 66, 67],\n",
      "          [68, 69, 70, 71],\n",
      "          [72, 73, 74, 75],\n",
      "          [76, 77, 78, 79]],\n",
      "\n",
      "         [[80, 81, 82, 83],\n",
      "          [84, 85, 86, 87],\n",
      "          [88, 89, 90, 91],\n",
      "          [92, 93, 94, 95]]]])\n",
      "tensor([[[[0]],\n",
      "\n",
      "         [[1]],\n",
      "\n",
      "         [[2]]],\n",
      "\n",
      "\n",
      "        [[[3]],\n",
      "\n",
      "         [[4]],\n",
      "\n",
      "         [[5]]]])\n",
      "torch.Size([2, 3, 4, 4])\n",
      "tensor([[[[  0,   0,   0,   0],\n",
      "          [  0,   0,   0,   0],\n",
      "          [  0,   0,   0,   0],\n",
      "          [  0,   0,   0,   0]],\n",
      "\n",
      "         [[ 16,  17,  18,  19],\n",
      "          [ 20,  21,  22,  23],\n",
      "          [ 24,  25,  26,  27],\n",
      "          [ 28,  29,  30,  31]],\n",
      "\n",
      "         [[ 64,  66,  68,  70],\n",
      "          [ 72,  74,  76,  78],\n",
      "          [ 80,  82,  84,  86],\n",
      "          [ 88,  90,  92,  94]]],\n",
      "\n",
      "\n",
      "        [[[144, 147, 150, 153],\n",
      "          [156, 159, 162, 165],\n",
      "          [168, 171, 174, 177],\n",
      "          [180, 183, 186, 189]],\n",
      "\n",
      "         [[256, 260, 264, 268],\n",
      "          [272, 276, 280, 284],\n",
      "          [288, 292, 296, 300],\n",
      "          [304, 308, 312, 316]],\n",
      "\n",
      "         [[400, 405, 410, 415],\n",
      "          [420, 425, 430, 435],\n",
      "          [440, 445, 450, 455],\n",
      "          [460, 465, 470, 475]]]])\n"
     ]
    }
   ],
   "source": [
    "# understanding of how matrix convolution and np.where works\n",
    "a = torch.arange(2*3*4*4).view(2, 3, 4, 4)\n",
    "print(a)\n",
    "b = torch.arange(2*3).view(2, 3, 1, 1)\n",
    "print(b)\n",
    "print((b*a).shape)\n",
    "print(b*a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
