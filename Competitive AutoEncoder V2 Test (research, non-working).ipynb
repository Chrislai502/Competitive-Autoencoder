{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41616e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_loader return dimensions:  torch.Size([3, 1, 28, 28])\n",
      "0 :\n",
      "tensor([9, 5, 5])\n",
      "1 :\n",
      "tensor([9, 5, 5])\n",
      "2 :\n",
      "tensor([9, 5, 5])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x144 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "''' Parameters (CHange Anything Here!) '''\n",
    "transform = transforms.ToTensor()\n",
    "batch_size = 3\n",
    "\n",
    "\n",
    "''' Code Starts Here '''\n",
    "#Data MNIST\n",
    "mnist_data = datasets.MNIST(root='./data', train = True, download = True, transform = transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset= mnist_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "dataiter = iter(data_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"Data_loader return dimensions: \", images.shape)\n",
    "input_size = images.shape[2]\n",
    "\n",
    "# plotting out the images\n",
    "plt.figure(figsize = (9,2)) # figsize(side spacing, height spacing (likely))\n",
    "plt.gray()\n",
    "for i, item in enumerate(images):\n",
    "    if i >= 9:\n",
    "        break\n",
    "    plt.subplot(2, 9, i + 1)\n",
    "    plt.imshow(item[0])\n",
    "\n",
    "for j, label in enumerate(labels):\n",
    "    print(j, \":\")\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35660444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :\n",
      "(2, 1, 3, 3)\n",
      "[[[[-0.30069196  0.11788306 -0.08021498]\n",
      "   [ 0.1447672  -0.21761513  0.2695081 ]\n",
      "   [-0.27041215  0.05243075 -0.00734672]]]\n",
      "\n",
      "\n",
      " [[[-0.25542682 -0.208678    0.08023819]\n",
      "   [-0.26417443 -0.05881071 -0.10150516]\n",
      "   [ 0.17048189  0.15289119  0.22528318]]]]\n",
      "2 :\n",
      "(2,)\n",
      "[-0.26474327 -0.04443625]\n"
     ]
    }
   ],
   "source": [
    "# testing model\n",
    "''' Conv 2d Layer \n",
    "#         Accessible Variables: .weights(Tensor), .bias(Tensor)\n",
    "#         parameters :\n",
    "#         torch.nn.Conv2d(in_channels, out_channels, \n",
    "#                         kernel_size, stride=1, padding=0, \n",
    "#                         dilation=1, groups=1, bias=True, \n",
    "#                         padding_mode='zeros')\n",
    "'''\n",
    "class Autoencoder_Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #Image size:N, 28, 28\n",
    "        self.conv1 = nn.Conv2d(1, 2, 3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(1, 2, 3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.conv1(x)\n",
    "        #encoded is the output of the layer\n",
    "        return encoded\n",
    "    \n",
    "model = Autoencoder_Test()\n",
    "\n",
    "generator = model.parameters() #(returns a generator)\n",
    "\n",
    "# for param in conv1.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# generator from model.params()\n",
    "# generator returns : filter weights in the first iteration, then the weights, then filter weights again\n",
    "# if there are 2 - (3x3) feature filters in the layer, the dimension of tensor returned will be: (2, 1, 3, 3)\n",
    "count = 0\n",
    "for params in generator:\n",
    "    count += 1\n",
    "    print(count, \":\")\n",
    "#     print(params)\n",
    "    npweight = params.detach().numpy()\n",
    "    print(npweight.shape)\n",
    "    print(npweight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf930945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Shape of the Prediction tensor : \n",
      " torch.Size([3, 2, 28, 28])\n",
      "tensor([[[[-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          ...,\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647]],\n",
      "\n",
      "         [[-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          ...,\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444]]],\n",
      "\n",
      "\n",
      "        [[[-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          ...,\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647]],\n",
      "\n",
      "         [[-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          ...,\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444]]],\n",
      "\n",
      "\n",
      "        [[[-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          ...,\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647],\n",
      "          [-0.2647, -0.2647, -0.2647,  ..., -0.2647, -0.2647, -0.2647]],\n",
      "\n",
      "         [[-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          ...,\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444],\n",
      "          [-0.0444, -0.0444, -0.0444,  ..., -0.0444, -0.0444, -0.0444]]]],\n",
      "       grad_fn=<MkldnnConvolutionBackward>)\n",
      "\n",
      "\n",
      " Sum of the feature maps(Should have batch_size values): \n",
      " tensor([[64.6628],\n",
      "        [98.4039],\n",
      "        [76.2235]])\n",
      "\n",
      "\n",
      " Maximum of the feature maps and index: \n",
      " tensor([98.4039]) ,  tensor([1])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e49c7757ce56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# if there is more than one value in this list, then backprop have to iterate through the list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwinnersMap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mwinnersMap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax_val\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mwinnersMap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax_val\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwinnersMap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_num' is not defined"
     ]
    }
   ],
   "source": [
    "# Batch Training loop\n",
    "# Hidden Units here is to be defined as feature maps\n",
    "# Spatial Sparsity: For every feature Filter, after batch prediction, pick the highest output activity and set the rest to 0\n",
    "# Lifetime Sparsity: For every feature Filter, after batch prediction, pick the hightst k% of all the filters picked in Spatial Sparsity\n",
    "from sortedcontainers import SortedList, SortedDict\n",
    "\n",
    "num_epochs = 1\n",
    "sorted_list = SortedList()\n",
    "winnersMap = {}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "#     for (img, labels) in data_loader:\n",
    "    img, labels = dataiter.next()\n",
    "    feature_map = model(img) # returns the feature maps of all batch examples in order\n",
    "    \n",
    "#     for feature in range(feature_map.shape[1]): # pick the feature element to iterate through\n",
    "#         for curr_batch in range(feature_map.shape[0]): # pick batch to iterate through\n",
    "# #     print (img.shape)\n",
    "# #     print(\"\\n Before reshaping dataLoader imgs :\\n\", img)\n",
    "# #     print(\"\\n After reshaping dataLoader imgs :\\n\", img.view(img.shape[1], batch_size, input_size, input_size))\n",
    "\n",
    "#     featur_map_mod = feature_map.view(feature)\n",
    "#     print(\"\\n Before reshaping prediction :\\n\", feature_map(feature_map.shape[1], batch_size, input_size, input_size)\n",
    "#     print(\"\\n Shape of the Prediction tensor after .view : \\n\", feature_map.view(feature_map.shape[1], batch_size, input_size, input_size).shape)\n",
    "#     print(\"\\n After reshaping prediction imgs :\\n\", feature_map.view(feature_map.shape[1], batch_size, input_size, input_size))\n",
    "\n",
    "#     # Checking if the feature maps are the same before and after modifying the rows\n",
    "#     for j, label in enumerate(labels):\n",
    "#     print(j, \":\")\n",
    "#     print(labels)\n",
    "    \n",
    "    # Summing up the activation maps to find the maximum activation hidden map from the the batch\n",
    "    summation = torch.sum(img, (2, 3)) # reduce the 3rd and 4th dimension of the tensor. Summation is a 2-dim tensor\n",
    "    print(\"\\n\\n Sum of the feature maps(Should have batch_size values): \\n\", summation)\n",
    "    \n",
    "    # batch_idx: torch tensor with the max batch index, size = num_features\n",
    "    # max_val:   torch tensor with the max_val for each batch, size = num_features\n",
    "    max_val, batch_idx = torch.max(summation, 0) # returns a tensor with the size of number of features\n",
    "    print(\"\\n\\n Maximum of the feature maps and index: \\n\", max_val, \", \", batch_idx)\n",
    "    \n",
    "    # where feature_num starts from 0\n",
    "    for feature_num, max_values in enumerate(max_val):\n",
    "    # Have to store list of tuples in sorted dict where tuples = (feature no., index)\n",
    "    # if there is more than one value in this list, then backprop have to iterate through the list\n",
    "        if winnersMap.get(max_val) == None:\n",
    "            winnersMap[max_val] = [(batch_idx[feature_num], feature_num)]\n",
    "        else:\n",
    "            winnersMap[max_val] = winnersMap[max_val].append((batch_idx, feature_num))\n",
    "    \n",
    "    sorted_dict = SortedDict(winnersMap) # store and the keys sort Automatically\n",
    "    \n",
    "    # when updating weights, only take the top k% within the sorted list and find their locations in model somehow\n",
    "    # set the rest of the require_grads to be False\n",
    "    # compute grad and update weights\n",
    "    # Set all the require_grads back to be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf3b236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11],\n",
      "          [12, 13, 14, 15]],\n",
      "\n",
      "         [[16, 17, 18, 19],\n",
      "          [20, 21, 22, 23],\n",
      "          [24, 25, 26, 27],\n",
      "          [28, 29, 30, 31]]],\n",
      "\n",
      "\n",
      "        [[[32, 33, 34, 35],\n",
      "          [36, 37, 38, 39],\n",
      "          [40, 41, 42, 43],\n",
      "          [44, 45, 46, 47]],\n",
      "\n",
      "         [[48, 49, 50, 51],\n",
      "          [52, 53, 54, 55],\n",
      "          [56, 57, 58, 59],\n",
      "          [60, 61, 62, 63]]],\n",
      "\n",
      "\n",
      "        [[[64, 65, 66, 67],\n",
      "          [68, 69, 70, 71],\n",
      "          [72, 73, 74, 75],\n",
      "          [76, 77, 78, 79]],\n",
      "\n",
      "         [[80, 81, 82, 83],\n",
      "          [84, 85, 86, 87],\n",
      "          [88, 89, 90, 91],\n",
      "          [92, 93, 94, 95]]]])\n",
      "tensor([[ 120,  376],\n",
      "        [ 632,  888],\n",
      "        [1144, 1400]])\n",
      "tensor([1144, 1400]) tensor([2, 2])\n"
     ]
    }
   ],
   "source": [
    "''' Testing Block '''\n",
    "# b = torch.arange(4 * 5 * 6).view(4, 5, 6) # .view(n-dim_size,.... 3rd-dim_size, 2nd-dim_size, 1st-dim_size)\n",
    "# # print(torch.arange(4*5*6))\n",
    "# print(b)\n",
    "# torch.sum(b, (2, 1))\n",
    "\n",
    "# print(\"\\n1st-dim reduction\\n\", torch.sum(b, 2)) # Summation over the 3rd-dimension: aka x-axis\n",
    "# print(\"\\n2nd-dim reduction\\n\", torch.sum(b, 1)) # Summation over the 2nd-dimension: aka y-axis\n",
    "# print(\"\\n3rd-dim reduction\\n\", torch.sum(b, 0)) # Summation over the 1st-dimension: aka Z-axis\n",
    "\n",
    "# # print(torch.sum(b, (0, 0))) # Throws an error, dimensions not repeatable\n",
    "# print(\"\\n2nd, 1st reduction\\n\", torch.sum(b, (0, 1))) # Reduced the 2nd and 1st dimension\n",
    "# print(\"\\n2nd, 1st reduction\\n\", torch.sum(b, (1, 0))) # Reduced the 2nd and 1st dimension, the same. Order Doesn't matter\n",
    "\n",
    "# print(\"\\n2nd, 1st reduction\\n\", torch.sum(b, (0, 1))) # Reduced the 2nd and 1st dimension\n",
    "# print(\"\\n1st, 3rd reduction\\n\", torch.sum(b, (0, 2))) # Reduced the 1st and 3rd dimension\n",
    "# print(\"\\n2nd, 3rd reduction\\n\", torch.sum(b, (1, 2))) # Reduced the 2nd and 3rd dimension\n",
    "\n",
    "# Strategy to reduce feature map tensor into Maximum Winners tensor\n",
    "img = torch.arange(3*2*4*4).view(3, 2, 4, 4)\n",
    "print(img)\n",
    "summation = torch.sum(img, (2, 3))\n",
    "print(summation)\n",
    "max_val, batch_idx = torch.max(summation, 0)\n",
    "print(max_val, batch_idx) # FOUNd Winner tensors and index!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbfba60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
      "         [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]],\n",
      "\n",
      "        [[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47],\n",
      "         [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]],\n",
      "\n",
      "        [[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n",
      "         [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]]])\n",
      "tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11],\n",
      "          [12, 13, 14, 15]],\n",
      "\n",
      "         [[16, 17, 18, 19],\n",
      "          [20, 21, 22, 23],\n",
      "          [24, 25, 26, 27],\n",
      "          [28, 29, 30, 31]]],\n",
      "\n",
      "\n",
      "        [[[32, 33, 34, 35],\n",
      "          [36, 37, 38, 39],\n",
      "          [40, 41, 42, 43],\n",
      "          [44, 45, 46, 47]],\n",
      "\n",
      "         [[48, 49, 50, 51],\n",
      "          [52, 53, 54, 55],\n",
      "          [56, 57, 58, 59],\n",
      "          [60, 61, 62, 63]]],\n",
      "\n",
      "\n",
      "        [[[64, 65, 66, 67],\n",
      "          [68, 69, 70, 71],\n",
      "          [72, 73, 74, 75],\n",
      "          [76, 77, 78, 79]],\n",
      "\n",
      "         [[80, 81, 82, 83],\n",
      "          [84, 85, 86, 87],\n",
      "          [88, 89, 90, 91],\n",
      "          [92, 93, 94, 95]]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "        [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "         50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63],\n",
      "        [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "         82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]])\n"
     ]
    }
   ],
   "source": [
    "# .view experimentation\n",
    "import torch\n",
    "img = torch.arange(3*16*2).view(3,2,16)\n",
    "print(img)\n",
    "\n",
    "input = img.view(3, 2, 4, 4)\n",
    "print(input)\n",
    "decoded_input = input.view(-1, 2* 4* 4)\n",
    "print(decoded_input)\n",
    "print(decoded_input.view(3,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
